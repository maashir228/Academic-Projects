{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLP Project\n",
    "\n",
    "## Group Members:\n",
    "\n",
    "* Fouzan Asif (19K-1345) (BCS-8A)\n",
    "* Aashir (19K-0314) (BCS-8A)\n",
    "* Abdul Saboor (19K-1433) (BCS-8A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Obtain images and labels for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_1624\\2523272197.py:44: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_images = np.array(train_images)\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_1624\\2523272197.py:46: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val_images = np.array(val_images)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = \"VOC2008\"\n",
    "\n",
    "def get_class_label(filename):\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    for obj in root.findall('object'):\n",
    "        name = obj.find('name').text\n",
    "        return name\n",
    "\n",
    "with open(os.path.join(dataset_path, \"ImageSets/Main/train.txt\"), 'r') as f:\n",
    "    train_image_names = f.readlines()\n",
    "train_image_names = [name.strip() for name in train_image_names]\n",
    "\n",
    "with open(os.path.join(dataset_path, \"ImageSets/Main/val.txt\"), 'r') as f:\n",
    "    val_image_names = f.readlines()\n",
    "val_image_names = [name.strip() for name in val_image_names]\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "val_images = []\n",
    "val_labels = []\n",
    "\n",
    "for name in train_image_names:\n",
    "    image_path = os.path.join(dataset_path, \"JPEGImages\", name + \".jpg\")\n",
    "    image = cv2.imread(image_path)\n",
    "    train_images.append(image)\n",
    "    annotation_path = os.path.join(dataset_path, \"Annotations\", name + \".xml\")\n",
    "    class_label = get_class_label(annotation_path)\n",
    "    train_labels.append(class_label)\n",
    "\n",
    "for name in val_image_names:\n",
    "    image_path = os.path.join(dataset_path, \"JPEGImages\", name + \".jpg\")\n",
    "    image = cv2.imread(image_path)\n",
    "    val_images.append(image)\n",
    "    annotation_path = os.path.join(dataset_path, \"Annotations\", name + \".xml\")\n",
    "    class_label = get_class_label(annotation_path)\n",
    "    val_labels.append(class_label)\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "val_images = np.array(val_images)\n",
    "val_labels = np.array(val_labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Mapping class labels to a range of integers for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "dataset_path = \"VOC2008\"\n",
    "\n",
    "class_names = np.unique(train_labels)\n",
    "class_map = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "train_labels = np.array([class_map[label] for label in train_labels])\n",
    "\n",
    "num_classes = len(class_names)\n",
    "train_labels = to_categorical(train_labels, num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Resizing training and validation (testing) images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "height = 224\n",
    "width = 224\n",
    "\n",
    "train_images_resized = []\n",
    "for image_path in train_image_names:\n",
    "    image = cv2.imread(\"VOC2008/JPEGImages/\"+image_path+\".jpg\")\n",
    "    image = cv2.resize(image, (height, width))\n",
    "    train_images_resized.append(image)\n",
    "\n",
    "train_images_res = np.array(train_images_resized)\n",
    "train_images_res = np.reshape(train_images_res, (len(train_images_res), height, width, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images_res = []\n",
    "for image_path in val_image_names:\n",
    "    image = cv2.imread(\"VOC2008/JPEGImages/\"+image_path+\".jpg\")\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    val_images_res.append(image)\n",
    "val_images_res = np.array(val_images_res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Defining Necessary functions for Accuracy and MPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def mAP(val_labels, val_preds, inverted_class_map):\n",
    "    ap_dict = {}\n",
    "    for i in range(len(inverted_class_map)):\n",
    "        class_name = inverted_class_map[i]\n",
    "        if class_name not in val_labels:\n",
    "            continue\n",
    "        y_true = (val_labels == class_name).astype(int)\n",
    "        y_pred = val_preds[:, i]\n",
    "        ap = average_precision_score(y_true, y_pred)\n",
    "        ap_dict[class_name] = ap\n",
    "    mAP = sum(ap_dict.values()) / len(ap_dict)\n",
    "    return mAP, ap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_class_map = dict(map(reversed, class_map.items()))\n",
    "\n",
    "def accuracy(model):\n",
    "    global class_map, val_images_res, val_labels\n",
    "    val_preds = model.predict(val_images_res)\n",
    "    c = 0\n",
    "    for i in range(len(val_images_res)):\n",
    "        if inverted_class_map[np.argmax(val_preds[i])] == val_labels[i]:\n",
    "            c+=1\n",
    "    Map, Ap = mAP(val_labels,val_preds, inverted_class_map)\n",
    "    return \"Accuracy: \" + str((c/len(val_images_res))*100) + \"%\\nMean Average Precision: \" + str(Map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"VGG_training_output.npy\",m1output)\n",
    "np.save(\"ResNet_training_output.npy\",m2output)\n",
    "np.save(\"DenseNet_training_output.npy\",m3output)\n",
    "np.save(\"MobileNet_training_output.npy\",m4output)\n",
    "np.save(\"InceptionV3_training_output.npy\",m5output)\n",
    "\n",
    "np.save(\"VGG_validation_output.npy\",v1output)\n",
    "np.save(\"ResNet_validation_output.npy\",v2output)\n",
    "np.save(\"DenseNet_validation_output.npy\",v3output)\n",
    "np.save(\"MobileNet_validation_output.npy\",v4output)\n",
    "np.save(\"InceptionV3_validation_output.npy\",v5output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1output = np.load(\"VGG_training_output.npy\",allow_pickle=True)\n",
    "m2output = np.load(\"ResNet_training_output.npy\",allow_pickle=True)\n",
    "m3output = np.load(\"DenseNet_training_output.npy\",allow_pickle=True)\n",
    "m4output = np.load(\"MobileNet_training_output.npy\",allow_pickle=True)\n",
    "m5output = np.load(\"InceptionV3_training_output.npy\",allow_pickle=True)\n",
    "\n",
    "v1output = np.load(\"VGG_validation_output.npy\",allow_pickle=True)\n",
    "v2output = np.load(\"ResNet_validation_output.npy\",allow_pickle=True)\n",
    "v3output = np.load(\"DenseNet_validation_output.npy\",allow_pickle=True)\n",
    "v4output = np.load(\"MobileNet_validation_output.npy\",allow_pickle=True)\n",
    "v5output = np.load(\"InceptionV3_validation_output.npy\",allow_pickle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN - 1: VGG-16 (with a 3 layer NN for classification)\n",
    "\n",
    "The fully connected NN has filtered 25000+ (or filtered 4096) input nodes, 1024 hidden layer nodes, and 20 output nodes\n",
    "\n",
    "### Step 4a: Defining VGG and the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "vgg_nn = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False #we don't trained pre-trained CNNs\n",
    "\n",
    "vgg_nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining checkpoints to store .h5 files per epoch (containing full model and weights only as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "arch_path = 'voc_new.h5'\n",
    "weights_path = 'voc_weights_new.h5'\n",
    "\n",
    "arch_checkpoint = ModelCheckpoint(arch_path, save_best_only=False, mode='min', save_weights_only=False)\n",
    "\n",
    "weights_checkpoint = ModelCheckpoint(weights_path, save_best_only=False, mode='min', save_weights_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4b: Training the model with VGG as CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "66/66 [==============================] - 439s 7s/step - loss: 27.1707 - accuracy: 0.4505\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 453s 7s/step - loss: 0.8457 - accuracy: 0.7954\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 459s 7s/step - loss: 0.3738 - accuracy: 0.8996\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 464s 7s/step - loss: 0.1651 - accuracy: 0.9498\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 462s 7s/step - loss: 0.1045 - accuracy: 0.9706\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 460s 7s/step - loss: 0.0897 - accuracy: 0.9811\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 456s 7s/step - loss: 0.0488 - accuracy: 0.9863\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 492s 7s/step - loss: 0.0606 - accuracy: 0.9905\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 480s 7s/step - loss: 0.0163 - accuracy: 0.9948\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 480s 7s/step - loss: 0.0147 - accuracy: 0.9962\n"
     ]
    }
   ],
   "source": [
    "history = vgg_nn.fit(train_images_res, train_labels, batch_size=32, epochs=10, callbacks=[arch_checkpoint, weights_checkpoint])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4b Alternate: Loading our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "vgg_nn = load_model(\"voc_new.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4c: Finding Accuracy and Mean Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 537s 8s/step\n",
      "Accuracy:61.45880234128771%\n",
      "Mean Average Precision: 0.48199936721385106\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(vgg_nn))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN - 2: ResNet50 with a custom 3 layer NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step a: Using Transfer Learning, importing a pre-trained CNN ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "for layers in base_model.layers:\n",
    "    layers.trainable = False\n",
    "    \n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "\n",
    "resnet_nn = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# model.load_weights('voc_weights.h5', by_name=True, skip_mismatch=True)\n",
    "\n",
    "resnet_nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Checkpoints per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "arch_path = 'resnet.h5'\n",
    "weights_path = 'resnet_weights_new.h5'\n",
    "\n",
    "arch_checkpoint = ModelCheckpoint(arch_path, save_best_only=False, mode='min', save_weights_only=False)\n",
    "\n",
    "weights_checkpoint = ModelCheckpoint(weights_path, save_best_only=False, mode='min', save_weights_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "66/66 [==============================] - 189s 3s/step - loss: 1.7291 - accuracy: 0.6097\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 184s 3s/step - loss: 0.7272 - accuracy: 0.7726\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 184s 3s/step - loss: 0.4548 - accuracy: 0.8550\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 184s 3s/step - loss: 0.2858 - accuracy: 0.9090\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 187s 3s/step - loss: 0.1859 - accuracy: 0.9488\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 190s 3s/step - loss: 0.0992 - accuracy: 0.9773\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 191s 3s/step - loss: 0.0570 - accuracy: 0.9915\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 193s 3s/step - loss: 0.0308 - accuracy: 0.9976\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 189s 3s/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 195s 3s/step - loss: 0.0095 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "resnet = resnet_nn.fit(train_images_res, train_labels, batch_size=32, epochs=10, callbacks=[arch_checkpoint, weights_checkpoint])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the saved and trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/70 [=========================>....] - ETA: 38s"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "resnet_nn = load_model(\"resnet.h5\")\n",
    "\n",
    "print(accuracy(resnet_nn))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN - 3: DenseNet with a custom 3 layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "densenet_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in densenet_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = densenet_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "densenet_nn = Model(inputs=densenet_model.input, outputs=predictions)\n",
    "\n",
    "densenet_nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "arch_path = 'densenet.h5'\n",
    "weights_path = 'densenet_weights_new.h5'\n",
    "\n",
    "arch_checkpoint = ModelCheckpoint(arch_path, save_best_only=False, mode='min', save_weights_only=False)\n",
    "\n",
    "weights_checkpoint = ModelCheckpoint(weights_path, save_best_only=False, mode='min', save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "66/66 [==============================] - 225s 3s/step - loss: 5.1220 - accuracy: 0.2207\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 215s 3s/step - loss: 2.2122 - accuracy: 0.3444\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 214s 3s/step - loss: 1.9868 - accuracy: 0.3984\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 217s 3s/step - loss: 1.8241 - accuracy: 0.4192\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 216s 3s/step - loss: 1.6800 - accuracy: 0.4694\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 215s 3s/step - loss: 1.4752 - accuracy: 0.5263\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 210s 3s/step - loss: 1.3781 - accuracy: 0.5623\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 205s 3s/step - loss: 1.2656 - accuracy: 0.5902\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 207s 3s/step - loss: 1.2099 - accuracy: 0.5936\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 200s 3s/step - loss: 1.0206 - accuracy: 0.6727\n"
     ]
    }
   ],
   "source": [
    "densenet = densenet_nn.fit(train_images_res, train_labels, batch_size=32, epochs=10, callbacks=[arch_checkpoint, weights_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 237s 3s/step\n",
      "Accuracy: 37.10040522287258%\n",
      "Mean Average Precision: 0.23809376655205425\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "densenet_nn = load_model(\"densenet.h5\")\n",
    "\n",
    "print(accuracy(densenet_nn))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN - 4: MobileNet with a custom 3 layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "mobilenet_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in mobilenet_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(mobilenet_model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "mobilenet_nn = Model(inputs=mobilenet_model.input, outputs=predictions)\n",
    "\n",
    "mobilenet_nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.load_weights('voc_weights.h5', by_name=True, skip_mismatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "arch_path = 'mobilenet.h5'\n",
    "weights_path = 'mobilenet_weights.h5'\n",
    "\n",
    "arch_checkpoint = ModelCheckpoint(arch_path, save_best_only=False, mode='min', save_weights_only=False)\n",
    "\n",
    "weights_checkpoint = ModelCheckpoint(weights_path, save_best_only=False, mode='min', save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "66/66 [==============================] - 61s 829ms/step - loss: 2.6092 - accuracy: 0.2724\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 58s 885ms/step - loss: 2.1162 - accuracy: 0.3648\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 58s 882ms/step - loss: 1.9177 - accuracy: 0.4173\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 61s 930ms/step - loss: 1.7715 - accuracy: 0.4552\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 58s 877ms/step - loss: 1.6296 - accuracy: 0.4931\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 57s 867ms/step - loss: 1.4860 - accuracy: 0.5429\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 59s 897ms/step - loss: 1.3865 - accuracy: 0.5727\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 55s 841ms/step - loss: 1.2911 - accuracy: 0.6035\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 59s 895ms/step - loss: 1.1261 - accuracy: 0.6461\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 57s 864ms/step - loss: 1.0815 - accuracy: 0.6731\n"
     ]
    }
   ],
   "source": [
    "mobilenet = mobilenet_nn.fit(train_images_res, train_labels, batch_size=32, epochs=10, callbacks=[arch_checkpoint, weights_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 64s 904ms/step\n",
      "Accuracy: 36.380009004952726%\n",
      "Mean Average Precision: 0.2121310724688476\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "mobilenet_nn = load_model(\"mobilenet.h5\")\n",
    "\n",
    "print(accuracy(mobilenet_nn))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN - 5: InceptionV3 with a custom 3 layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inceptionv3_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in inceptionv3_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(inceptionv3_model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "icpv3_nn = Model(inputs=inceptionv3_model.input, outputs=predictions)\n",
    "\n",
    "icpv3_nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "arch_path = 'icpv3.h5'\n",
    "weights_path = 'icpv3_weights.h5'\n",
    "\n",
    "arch_checkpoint = ModelCheckpoint(arch_path, save_best_only=False, mode='min', save_weights_only=False)\n",
    "\n",
    "weights_checkpoint = ModelCheckpoint(weights_path, save_best_only=False, mode='min', save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "66/66 [==============================] - 98s 1s/step - loss: 32.9993 - accuracy: 0.1227\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 101s 2s/step - loss: 3.4303 - accuracy: 0.2075\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 104s 2s/step - loss: 2.8467 - accuracy: 0.2255\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 104s 2s/step - loss: 2.5857 - accuracy: 0.2639\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 104s 2s/step - loss: 2.4524 - accuracy: 0.2984\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 117s 2s/step - loss: 2.4034 - accuracy: 0.3051\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 108s 2s/step - loss: 2.3784 - accuracy: 0.3122\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 104s 2s/step - loss: 2.3105 - accuracy: 0.3112\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 106s 2s/step - loss: 2.2761 - accuracy: 0.3126\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 104s 2s/step - loss: 2.2334 - accuracy: 0.3254\n"
     ]
    }
   ],
   "source": [
    "icpv3 = icpv3_nn.fit(train_images_res, train_labels, batch_size=32, epochs=10, callbacks=[arch_checkpoint, weights_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 121s 2s/step\n",
      "Accuracy: 28.95092300765421%\n",
      "Mean Average Precision: 0.09713809770477483\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "icpv3_nn = load_model(\"icpv3.h5\")\n",
    "\n",
    "print(accuracy(icpv3_nn))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJECT (C): ENSEMBLED LEARNING\n",
    "\n",
    "### Early Fusion & Late Fusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model1 = load_model('voc_new.h5')\n",
    "model2 = load_model('resnet.h5')\n",
    "model3 = load_model('mobilenet.h5')\n",
    "model4 = load_model('densenet.h5')\n",
    "model5 = load_model('icpv3.h5')\n",
    "\n",
    "input1 = model1.input\n",
    "input2 = model2.input\n",
    "input3 = model3.input\n",
    "# input4 = model4.input\n",
    "input5 = model5.input\n",
    "\n",
    "output1 = model1.layers[-2].output\n",
    "output2 = model2.layers[-2].output\n",
    "output3 = model3.layers[-2].output\n",
    "# output4 = model4.layers[-2].output\n",
    "output5 = model5.layers[-2].output\n",
    "\n",
    "merged = Concatenate()([output1, output2, output3, output5])\n",
    "x = Dense(64, activation='relu')(merged)\n",
    "x = Dense(20, activation='sigmoid')(x)\n",
    "new_model = Model(inputs=[model1.input, model2.input, model3.input, model5.input], outputs=x)\n",
    "\n",
    "new_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "66/66 [==============================] - 787s 12s/step - loss: 2.3518 - accuracy: 0.5580\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 794s 12s/step - loss: 1.2622 - accuracy: 0.7262\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 804s 12s/step - loss: 0.8612 - accuracy: 0.8162\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 798s 12s/step - loss: 0.4829 - accuracy: 0.8915\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Semester 8\\DLP\\Project\\Anothertry.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Semester%208/DLP/Project/Anothertry.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m new_model\u001b[39m.\u001b[39;49mfit([train_images_res, train_images_res, train_images_res, train_images_res], train_labels, \n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Semester%208/DLP/Project/Anothertry.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                         epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[arch_checkpoint, weights_checkpoint])\n",
      "File \u001b[1;32mf:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mf:\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mf:\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mf:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mf:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mf:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mf:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mf:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mf:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = new_model.fit([train_images_res, train_images_res, train_images_res, train_images_res], train_labels, \n",
    "                        epochs=10, batch_size=32, callbacks=[arch_checkpoint, weights_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 438s 7s/step\n",
      "66/66 [==============================] - 181s 3s/step\n",
      "66/66 [==============================] - 54s 812ms/step\n",
      "66/66 [==============================] - 200s 3s/step\n",
      "66/66 [==============================] - 107s 2s/step\n",
      "70/70 [==============================] - 461s 7s/step\n",
      "70/70 [==============================] - 209s 3s/step\n",
      "70/70 [==============================] - 56s 799ms/step\n",
      "70/70 [==============================] - 203s 3s/step\n",
      "70/70 [==============================] - 107s 2s/step\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"early_fusion.h5\")\n",
    "\n",
    "# Create a pipeline with SVM classifier\n",
    "svm = make_pipeline(StandardScaler(), SVC(kernel='linear', C=1.0))\n",
    "\n",
    "# Fit the SVM classifier on the outputs of the final layers of each model\n",
    "\n",
    "m1output = model1.predict(train_images_res)\n",
    "m2output = model2.predict(train_images_res)\n",
    "m3output = model3.predict(train_images_res)\n",
    "m4output = model4.predict(train_images_res)\n",
    "m5output = model5.predict(train_images_res)\n",
    "\n",
    "v1output = model1.predict(val_images_res)\n",
    "v2output = model2.predict(val_images_res)\n",
    "v3output = model3.predict(val_images_res)\n",
    "v4output = model4.predict(val_images_res)\n",
    "v5output = model5.predict(val_images_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 71.36425033768573%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "labels = []\n",
    "for name in train_image_names:\n",
    "    annotation_path = os.path.join(dataset_path, \"Annotations\", name + \".xml\")\n",
    "    class_label = get_class_label(annotation_path)\n",
    "    labels.append(class_label)\n",
    "labels = np.array(labels)\n",
    "\n",
    "train_features = np.concatenate((m1output, m2output, m3output, m5output), axis=1)\n",
    "val_features = np.concatenate((v1output,v2output,v3output,v5output), axis=1)\n",
    "\n",
    "svm.fit(train_features, labels)\n",
    "\n",
    "val_preds = svm.predict(val_features)\n",
    "print(\"Accuracy on validation set: \" + str(accuracy_score(val_labels, val_preds)*100) + \"%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = np.mean([v1output,v2output,v3output,v4output,v5output],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.mean([v1output,v2output,v3output,v4output,v5output],axis=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Late Fusion Accuracy: 68.03241782980639%\n"
     ]
    }
   ],
   "source": [
    "abcd = np.argmax(abc, axis=1)\n",
    "lst = []\n",
    "for all in abcd:\n",
    "    lst.append(inverted_class_map[all])\n",
    "\n",
    "c=0\n",
    "for all in range(len(lst)):\n",
    "    if lst[all] == val_labels[all]:\n",
    "        c+=1\n",
    "\n",
    "print(\"Late Fusion Accuracy: \" + str(100*(c/len(val_labels))) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
